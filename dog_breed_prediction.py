# -*- coding: utf-8 -*-
"""dog breed prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k4q6VPiAjuQlNcnGMEC4GT_cW2kOfmqY
"""

import os
os.path.isdir('/root/.kaggle/')

# Uploading the kaggle json file
from google.colab import files
files.upload()

## installing the kaggle api
!pip install -q kaggle

#create a directory to store the json file
!mkdir -p /root/.kaggle
!mv kaggle.json /root/.kaggle/

##change the permission to avoid kaggle warning
!chmod 600 /root/.kaggle/kaggle.json

# Commented out IPython magic to ensure Python compatibility.
# creating the directory and changing the current working directory
!mkdir dog_dataset
# %cd dog_dataset

#finding the dataset needed from kaggle
!kaggle datasets list -s dogbreedidfromcomp

# Commented out IPython magic to ensure Python compatibility.
#downloading datasets from kaggle
!kaggle datasets download catherinehorng/dogbreedidfromcomp
# %cd ..

#unzipping the downloaded data and removing unusable file
!unzip dog_dataset/dogbreedidfromcomp.zip -d dog_dataset
!rm dog_dataset/dogbreedidfromcomp.zip
!rm dog_dataset/sample_submission.csv

#importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from tqdm import tqdm
import keras.utils as image
from sklearn.preprocessing import label_binarize
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import Adam

!pip install --upgrade tensorflow
from tensorflow.keras.preprocessing.image import load_img

##reading the labels and checking shape of our data
dog_labels = pd.read_csv("dog_dataset/labels.csv")
print(dog_labels.shape)
dog_labels.head()

#counting the number of each breed in the dataset
breed = dog_labels["breed"]
breed_count = breed.value_counts()
breed_count.head()

#selecting the breeds. Due to computation power 
dog_names = ['scottish_deerhound', 'maltese_dog', 'bernese_mountain_dog']
labels = dog_labels[(dog_labels['breed'].isin(dog_names))]
labels = labels.reset_index()
labels.head()

##creating numpy matrix with zeros 
X_data = np.zeros((len(labels), 224, 224, 3), dtype='float32')

#one hot encoding
Y_data = label_binarize(labels['breed'], classes = dog_names)

#reading and coverting image to numpy array and normalizing the dataset
for i in tqdm(range(len(labels))):
  img = image.load_img('dog_dataset/train/%s.jpg' % labels['id'][i], target_size = (224,224))
  img = image.img_to_array(img)
  x = np.expand_dims(img.copy(), axis = 0)
  X_data[i] = x / 255.0

#printing train image and one hot encode shape and size
print('\nTrain Images Shape: ', X_data.shape, ' size: {:,}'.format(X_data.size))
print('\nOne-hot encoded output shape: ', Y_data.shape, ' size: {:,}'.format(Y_data.size))

#building the model
model = Sequential()

model.add(Conv2D(filters = 64, kernel_size=(5,5), activation='relu', input_shape = (224, 224,3)))
model.add(MaxPool2D(pool_size = (2,2)))

model.add(Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', kernel_regularizer = 'l2'))
model.add(MaxPool2D(pool_size = (2,2)))

model.add(Conv2D(filters = 16, kernel_size = (7,7), activation = 'relu', kernel_regularizer = 'l2'))
model.add(MaxPool2D(pool_size = (2,2)))

model.add(Conv2D(filters = 8, kernel_size = (5,5), activation = 'relu', kernel_regularizer = 'l2'))
model.add(MaxPool2D(pool_size = (2,2)))

model.add(Flatten())
model.add(Dense(128, activation = "relu", kernel_regularizer = 'l2'))
model.add(Dense(64, activation = "relu", kernel_regularizer = 'l2'))
model.add(Dense(len(dog_names), activation = "softmax"))

model.compile(loss = 'categorical_crossentropy', optimizer = Adam(0.0001), metrics = ['accuracy'])
model.summary()

#splitting data set into training and texting datasets
X_train_and_val, X_test, Y_train_and_val, Y_text = train_test_split(X_data, Y_data, test_size = 0.1)

#Splitting the training data set into training and validation datasets
X_train, X_val, Y_train, Y_val = train_test_split(X_train_and_val, Y_train_and_val, test_size = 0.2)

#training the model
epochs = 100
batch_size = 128

history = model.fit(X_train, Y_train, batch_size= batch_size, epochs = epochs, validation_data = (X_val, Y_val))

### Plot the training history
plt.figure(figsize=(12, 5))
plt.plot(history.history['accuracy'], color = 'r')
plt.plot(history.history['val_accuracy'], color = 'b')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend(['train', 'val'])

plt.show()

Y_pred = model.predict(X_test)
score = model.evaluate(X_test, Y_text)

print ('Accuracy over the test set: \n', round((score[1]*100), 2), '%')

#Plotting image to compare
plt.imshow(X_test[1,:,:,:])
plt.show()

#Finding max value from prediction list and comparing original value vs predicted
print("Original : ", labels['breed'][np.argmax(Y_text[1])])
print("Predicted : ", labels['breed'][np.argmax(Y_pred[1])])

model.save("dog_breed.h5")